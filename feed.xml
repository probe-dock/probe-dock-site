<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Probe Dock</title>
    <description>ProbeDock is a test tracking and analysis tool.
</description>
    <link>http://probedock.io/</link>
    <atom:link href="http://probedock.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 01 Mar 2016 21:03:25 +0100</pubDate>
    <lastBuildDate>Tue, 01 Mar 2016 21:03:25 +0100</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Probe Dock v0.1.23 Released</title>
        <description>&lt;p&gt;&lt;strong&gt;Probe Dock v0.1.23&lt;/strong&gt; is now available!
It brings you 3 new widgets to show new data about your projects, including the first 2 widgets to show data about individual tests.
There have also been several other improvements.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#widgets&quot;&gt;Widgets&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#category-bar-widget&quot;&gt;Category bar&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#test-results-history-widget&quot;&gt;Test results history&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#test-execution-time-widget&quot;&gt;Execution time&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#test-result-details&quot;&gt;Test result details&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#project-repository-link&quot;&gt;Project repository link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#improvements&quot;&gt;Improvements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bugfixes&quot;&gt;Bugfixes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;widgets&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;widgets&quot;&gt;Widgets&lt;/h2&gt;

&lt;p&gt;3 new widgets have been added, 1 in the organization dashboard and project details pages, and 2 in the test details page.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;category-bar-widget&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;category-bar&quot;&gt;Category bar&lt;/h3&gt;

&lt;p&gt;The first thing you now see in your &lt;strong&gt;organization’s dashboard&lt;/strong&gt; is a breakdown of the test categories used across all your projects:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/category-bar-widget.png&quot; alt=&quot;Category Bar Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The same widget is also included in the &lt;strong&gt;project details page&lt;/strong&gt; where it shows the breakdown for that particular project.
This information can also be &lt;strong&gt;filtered by user&lt;/strong&gt; so that you can see what kind of tests each developer most works with.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;test-results-history-widget&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;test-results-history&quot;&gt;Test results history&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;test details page&lt;/strong&gt; finally has widgets!
To access this page, click on a result in one of your test reports:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/test-report-results.png&quot; alt=&quot;Test Report Results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This first widget will show you &lt;strong&gt;the latest results of the test&lt;/strong&gt; in the same style as test reports,
allowing you to immediately identify &lt;strong&gt;flickering&lt;/strong&gt;, i.e. when a test passes, then fails, then passes, etc.
This could indicate an error due to leftover state data from another test or a random condition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/test-results-history-widget.png&quot; alt=&quot;Test Results History Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By default, results are shown &lt;strong&gt;across all versions&lt;/strong&gt; of the project.
Use the filters to see only the results received &lt;strong&gt;for a particular version&lt;/strong&gt;.
The widget will also tell you if there are versions in which the test &lt;strong&gt;was never run&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/test-results-history-widget-filtered.png&quot; alt=&quot;Test Results History Widget (Filtered)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can also filter the results &lt;strong&gt;by user&lt;/strong&gt;, which can help you identify issues on a given developer’s machine.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;test-execution-time-widget&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;test-execution-time&quot;&gt;Test execution time&lt;/h3&gt;

&lt;p&gt;This other widget in the test details page shows you the &lt;strong&gt;evolution of the test’s execution time&lt;/strong&gt;.
You will see if a test suddenly develops &lt;strong&gt;performance problems&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/test-execution-time-widget.png&quot; alt=&quot;Test Execution Time Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By default it shows the execution time for the 50 latest results, and then allows you to navigate backwards and forwards in time.
You can filter to only show the execution time for a given project version, or for a given runner (the user who ran the tests).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;test-result-details&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;test-result-details&quot;&gt;Test result details&lt;/h3&gt;

&lt;p&gt;Clicking on one of the results in the &lt;strong&gt;test results history widget&lt;/strong&gt; will open a dialog giving you &lt;strong&gt;more detailed information about the result&lt;/strong&gt;,
including for example the stack trace in case of failed tests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/test-result-details.png&quot; alt=&quot;Test result details&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;project-repository-link&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;project-repository-link&quot;&gt;Project repository link&lt;/h2&gt;

&lt;p&gt;You can now add links to your projects’ repositories on GitHub or wherever you’re hosting your source code:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/project-repo-link.png&quot; alt=&quot;Project repository link&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Convenient link icons will then appear in the projects list and on the project details page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/project-repo-link-button.png&quot; alt=&quot;Project repository link button&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that this should be the URL to access your source code in the browser (starting with &lt;code&gt;http://&lt;/code&gt; or &lt;code&gt;https://&lt;/code&gt;), not the clone/checkout URL.&lt;/p&gt;

&lt;p&gt;For now we only use this feature to display links, but it opens the door for us to start analyzing your code to enrich the Probe Dock experience.
For example, you will be able to see a failing test’s source code directly in Probe Dock,
or we will be able to correlate the growth of your code base to the results of your automated tests.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;improvements&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;improvements&quot;&gt;Improvements&lt;/h2&gt;

&lt;h3 id=&quot;collapsed-projects-in-the-recent-activity-widget&quot;&gt;Collapsed projects in the recent activity widget&lt;/h3&gt;

&lt;p&gt;If you publish reports with test results from many projects, only &lt;strong&gt;the first 5&lt;/strong&gt; will be shown by default with the rest collapsed.
The “More…” button allows you to show the rest.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/recent-activity-widget-collapsed-projects.png&quot; alt=&quot;Collapsed projects in the recent activity widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Additionally, in the project details page, only the version will be shown since you’re in the context of a particular project.&lt;/p&gt;

&lt;h3 id=&quot;gravatar-identicons&quot;&gt;Gravatar identicons&lt;/h3&gt;

&lt;p&gt;Probe Dock account avatars are powered by &lt;a href=&quot;http://gravatar.com&quot;&gt;Gravatar&lt;/a&gt;.
You can register your e-mail there and associate an image with it.&lt;/p&gt;

&lt;p&gt;However, for those who don’t want to do that, we have activated &lt;strong&gt;identicons&lt;/strong&gt;,
which means that their avatar will be a pattern generated from their e-mail.
Even users that do not use Gravatar will have different avatars from each other.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/gravatar-identicons.png&quot; alt=&quot;Gravatar identicons&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;copy-on-click-organization--project-ids&quot;&gt;Copy-on-click organization &amp;amp; project IDs&lt;/h3&gt;

&lt;p&gt;Most IDs can now be copied with a simple click:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/copy-on-click-project-id.png&quot; alt=&quot;Copy-on-click organization and project IDs&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;page-titles&quot;&gt;Page titles&lt;/h3&gt;

&lt;p&gt;You will see more useful page titles when navigating:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_23/page-titles.png&quot; alt=&quot;Page titles&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;other-improvements&quot;&gt;Other improvements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;In the projects list, the pie chart showing the results of the last test run is now a link to that test run.&lt;/li&gt;
  &lt;li&gt;We have a favicon, finally!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;bugfixes&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bugfixes&quot;&gt;Bugfixes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Fixed a bug where invite links to new organizations would not work when already logged in.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 01 Mar 2016 22:00:00 +0100</pubDate>
        <link>http://probedock.io/blog/releases/2016/03/01/probedock-v0.1.23/</link>
        <guid isPermaLink="true">http://probedock.io/blog/releases/2016/03/01/probedock-v0.1.23/</guid>
        
        
        <category>releases</category>
        
      </item>
    
      <item>
        <title>Probe Dock JUGL Presentation</title>
        <description>&lt;p&gt;We presented Probe Dock at &lt;a href=&quot;http://jugl.ch/&quot;&gt;JUGL&lt;/a&gt; this evening.
It went very well and we got interesting questions and feedback.
Thanks to all attendees and of course to organizers and &lt;a href=&quot;http://octo.ch&quot;&gt;OCTO&lt;/a&gt; for hosting the presentation!&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://www.dropbox.com/s/21tcoa2xn4ck5h2/Jugl%20February%202016.pdf?dl=0&quot;&gt;slides we used for the presentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As always, do not hesitate to contact us if you have additional questions or feedback.&lt;/p&gt;

&lt;h2 id=&quot;q--a-after-the-presentation&quot;&gt;Q &amp;amp; A after the presentation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Does Probe Dock obtain the Jenkins runner when a job is run?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Not yet. We plan to make a plugin for Jenkins with a deeper integration of Probe Dock.&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Feb 2016 21:00:00 +0100</pubDate>
        <link>http://probedock.io/blog/news/2016/02/04/jugl-presentation/</link>
        <guid isPermaLink="true">http://probedock.io/blog/news/2016/02/04/jugl-presentation/</guid>
        
        
        <category>news</category>
        
      </item>
    
      <item>
        <title>Probe Dock v0.1.14 Released</title>
        <description>&lt;p&gt;&lt;strong&gt;Probe Dock v0.1.14&lt;/strong&gt; is now available!
It brings you new widgets on your organization’s dashboard as well as on the new project page.
Also, if you have an existing testing infrastructure that generates standard xUnit XML test reports (like JUnit does),
Probe Dock can now collect and analyse those reports as well.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#xunit&quot;&gt;xUnit/JUnit XML Reports&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#new-pages&quot;&gt;New Pages&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#widgets&quot;&gt;Widgets&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#test-run-activity&quot;&gt;Test Run Activity&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#test-suite-size&quot;&gt;Test Suite Size&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#contributors&quot;&gt;Contributors&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#project-recent-activity&quot;&gt;Recent Activity for Projects&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#project-health&quot;&gt;Project Health&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#test-report-dropzone&quot;&gt;Test Report Dropzone&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bugfixes&quot;&gt;Bugfixes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;xunit&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;xunitjunit-xml-reports&quot;&gt;xUnit/JUnit XML Reports&lt;/h2&gt;

&lt;p&gt;The standard way of using Probe Dock is to integrate its &lt;a href=&quot;https://github.com/probedock/probedock-probes&quot;&gt;probes&lt;/a&gt; into your favorite test framework.
This allows Probe Dock to automatically collect information about your testing environment and activity,
and allows you to enrich your tests with various metadata like custom tags and tickets.&lt;/p&gt;

&lt;p&gt;However, if you already have a testing infrastructure that generates &lt;strong&gt;standard xUnit/JUnit XML reports&lt;/strong&gt;,
you can also submit these reports to Probe Dock directly.
Most test frameworks are therefore supported by Probe Dock even if we do not yet supply a probe,
as almost all frameworks can generate this format with some additional configuration (including Ruby and Node.js test frameworks, for example).&lt;/p&gt;

&lt;p&gt;If you have archived previous report files, you can also publish those to Probe Dock as they include a timestamp indicating when the tests were run.
Your can therefore import your past testing history.&lt;/p&gt;

&lt;p&gt;You can publish an XML test report &lt;a href=&quot;#xunit-manual&quot;&gt;manually&lt;/a&gt; for trial purposes,
or from a &lt;a href=&quot;#xunit-ci&quot;&gt;continuous integration environment&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;xunit-manual&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-to-publish-an-xml-test-report-manually&quot;&gt;How to publish an XML test report manually&lt;/h3&gt;

&lt;p&gt;If you do not already have a Probe Dock account and organization, you can &lt;a href=&quot;https://trial.probedock.io/register&quot;&gt;register a trial account&lt;/a&gt; to try it.
Also make sure you have created your project in Probe Dock.&lt;/p&gt;

&lt;p&gt;Log in to Probe Dock and go to your project’s page.&lt;/p&gt;

&lt;p&gt;Set the desired version and test category in the publishing widget,
then simply drag-and-drop your XML report file into the dropzone:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/xunit.png&quot; alt=&quot;xUnit XML Reports Dropzone&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If Probe Dock cannot understand the contents of your report file for any reason,
it will notify you and provide you with a list of errors describing what seems to be missing or invalid.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;xunit-ci&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-to-publish-an-xml-test-report-from-a-continuous-integration-environment&quot;&gt;How to publish an XML test report from a continuous integration environment&lt;/h3&gt;

&lt;p&gt;If you do not already have a Probe Dock account and organization, you can &lt;a href=&quot;https://trial.probedock.io/register&quot;&gt;register a trial account&lt;/a&gt; to try it.
Also make sure you have created your project in Probe Dock.&lt;/p&gt;

&lt;p&gt;Log in to Probe Dock and go to your organization’s page.
If you do not already have a technical user, go to the members page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/organization-members.png&quot; alt=&quot;Organization Members Page&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click on “Add a member” and create a &lt;strong&gt;technical user&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/new-technical-member.png&quot; alt=&quot;Create Technical User&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once you have your technical user, you can generate an &lt;strong&gt;API access token&lt;/strong&gt; for that user from the members page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/technical-user-api-access-token.png&quot; alt=&quot;Technical User API Access Token&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You also need to go to your project list or the project’s page and get the &lt;strong&gt;ID&lt;/strong&gt; of your project:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/project-id.png&quot; alt=&quot;Project ID&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, on a Unix continuous integration environment for example, you can simply curl your XML report file into Probe Dock.
You must supply headers for your API access token, the project ID, the project version, and optionally a test category:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat report.xml | curl --data-binary @- \
                      -H &quot;Content-Type: application/xml&quot; \
                      -H &quot;Authorization: Bearer YOUR_API_ACCESS_TOKEN_HERE&quot; \
                      -H &quot;Probe-Dock-Project-Id: o4m14w1u6hz3&quot; \
                      -H &quot;Probe-Dock-Project-Version: 1.0.0&quot; \
                      -H &quot;Probe-Dock-Category: JUnit&quot; \
                      https://trial.probedock.io/api/publish
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If your XML report file is accepted, the HTTP status code of the response will be &lt;em&gt;202 Accepted&lt;/em&gt;.
Your test report will appear in Probe Dock a few seconds later.&lt;/p&gt;

&lt;p&gt;If Probe Dock cannot understand the contents of your report file for any reason,
it will respond with the HTTP status code &lt;em&gt;422 Unprocessable Entity&lt;/em&gt; and provide you with a list of errors indicating what seems to be missing or invalid.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;new-pages&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;new-pages&quot;&gt;New Pages&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;project page&lt;/strong&gt; shows detailed information about each of your organization’s projects.
It includes all of the new &lt;a href=&quot;#widgets&quot;&gt;widgets&lt;/a&gt; described in this article.&lt;/p&gt;

&lt;p&gt;You can access it by clicking on a project’s name in the project list,
or by clicking on a project version label in a test run report or in the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/project-label.png&quot; alt=&quot;Project Label Link&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;test details page&lt;/strong&gt; will show information about a particular test in your test suite.
At this time it only includes basic details about a test such as the dates it was first and last run,
but it will soon allow you to browse the result of the tests or see how its execution time changed over time.
You can access it by clicking a test result’s name in a test run report.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;widgets&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;widgets&quot;&gt;Widgets&lt;/h2&gt;

&lt;p&gt;5 new widgets have been added and 1 was improved.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;test-run-activity&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;test-run-activity&quot;&gt;Test Run Activity&lt;/h3&gt;

&lt;p&gt;This widget is an updated version of the previous &lt;strong&gt;new tests&lt;/strong&gt; widget.
It not only allows you to see the number of new tests written by day, like before,
but you can also switch to the number of test runs by day.&lt;/p&gt;

&lt;p&gt;Each time a developer runs tests on his machine and publishes the results to Probe Dock,
or each time a continuous integration environment does the same, it counts for 1 test run.
The number of test runs by day therefore allows you to track the test execution activity,
not only when new tests are written.&lt;/p&gt;

&lt;p&gt;Of course, this information can still be filtered both by project and/or user.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/test-run-activity.png&quot; alt=&quot;Test Run Activity Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In addition to being in the dashboard, this widget has also been added to the new project page,
showing the activity specific to that project.&lt;/p&gt;

&lt;p&gt;A simplified version of the widget is also displayed in each project’s box in the project list:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/test-run-activity-project-list.png&quot; alt=&quot;Test Run Activity Widget in the Project List&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;test-suite-size&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;test-suite-size&quot;&gt;Test Suite Size&lt;/h3&gt;

&lt;p&gt;This widget shows the evolution of the total number of tests in your test suite by week.
Unlike the test run activity widget which shows daily activity, this one shows the cumulative growth of the test suite, and over a much longer timespan.
Of course, it can also be filtered by project and/or user.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/test-suite-size.png&quot; alt=&quot;Test Suite Size Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It has been added both to the dashboard and the new project page.
In the project page, it only shows the total number of the tests specific to the project.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;contributors&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;contributors&quot;&gt;Contributors&lt;/h3&gt;

&lt;p&gt;This widget lists the organization members who have contributed to the testing effort by writing new tests.
It also indicates how many and what kinds of tests they have written, which shows who is specialized in what.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/contributors.png&quot; alt=&quot;Contributors Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This widget has been added both to the dashboard and the new project page.
In the project page, it only shows the organization members who have written new tests for a specific project.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;project-recent-activity&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;recent-activity-for-projects&quot;&gt;Recent Activity for Projects&lt;/h3&gt;

&lt;p&gt;This widget is similar to the &lt;strong&gt;recent activity&lt;/strong&gt; widget on the dashboard, but only shows the activity for a specific project in the new project page.
In addition to listing the latest test runs, it also indicates if new tests were added for each test run, and how many.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/project-recent-activity.png&quot; alt=&quot;Project Recent Activity Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;project-health&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;project-health&quot;&gt;Project Health&lt;/h3&gt;

&lt;p&gt;This widget shows the latest state of a project for a specific version.
For each version, it indicates how many tests there are in the project, and how many were passing, failing or inactive the last time they were run.
This gives you a quick view on the overall health of a project.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/project-health.png&quot; alt=&quot;Project Health Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;test-report-dropzone&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;test-report-dropzone&quot;&gt;Test Report Dropzone&lt;/h3&gt;

&lt;p&gt;If you want to quickly try Probe Dock and your favorite test framework generates standard xUnit/JUnit XML test reports,
you can simply take your XML report file and drag-and-drop it into this widget.
Probe Dock will queue it for processing and your report will appear a few seconds later.&lt;/p&gt;

&lt;p&gt;You can select the version of the project used to run the tests, as well as the test category:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_14/xunit.png&quot; alt=&quot;xUnit XML Reports Dropzone&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As described &lt;a href=&quot;#xunit&quot;&gt;at the beginning of the article&lt;/a&gt;,
you do not have to manually upload XML reports:
they can also be sent directly to the Probe Dock API.
The widget is provided because it is convenient for trial purposes.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;bugfixes&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bugfixes&quot;&gt;Bugfixes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Tags and tickets were not correctly assigned to tests when test results were published.&lt;/li&gt;
  &lt;li&gt;Multiple test results sharing names and test keys caused payload processing to fail in special cases.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 02 Feb 2016 20:00:00 +0100</pubDate>
        <link>http://probedock.io/blog/releases/2016/02/02/probedock-v0.1.14/</link>
        <guid isPermaLink="true">http://probedock.io/blog/releases/2016/02/02/probedock-v0.1.14/</guid>
        
        
        <category>releases</category>
        
      </item>
    
      <item>
        <title>Probe Dock at the JUGL</title>
        <description>&lt;p&gt;We will present Probe Dock on February 4th, 6:30pm! The &lt;a href=&quot;http://jugl/&quot;&gt;JUGL&lt;/a&gt; is a Java user group based in Lausanne, Switzerland. The goal is to facilitate the discussions on the usage and evolution of Java and to establish relations between developers, architects or any kind of people interested by Java technology.&lt;/p&gt;

&lt;p&gt;The title of our presentation is: &lt;strong&gt;&lt;a href=&quot;http://jugl.ch/2016/02/04/ProbeDock.html&quot;&gt;Probe Dock - Faites parler vos tests automatisés&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The presentation will be held in the offices of &lt;a href=&quot;http://octo.ch&quot;&gt;Octo&lt;/a&gt;, Tour de la mobilière, avenue du Théâtre 7, Lausanne on the second floor.&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Jan 2016 21:00:00 +0100</pubDate>
        <link>http://probedock.io/blog/news/2016/01/18/probedock-at-the-jugl/</link>
        <guid isPermaLink="true">http://probedock.io/blog/news/2016/01/18/probedock-at-the-jugl/</guid>
        
        
        <category>news</category>
        
      </item>
    
      <item>
        <title>Probe Dock v0.1.11 Released</title>
        <description>&lt;p&gt;&lt;strong&gt;Probe Dock v0.1.11&lt;/strong&gt; is now available!
It brings you the ability to build &lt;strong&gt;report permalinks&lt;/strong&gt; and new features for Java probes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#report-permalinks&quot;&gt;Report permalinks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#junit-assumptions-and-ignored-tests&quot;&gt;JUnit Assumptions Support &amp;amp; Ignored Tests&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#rt-arquillian&quot;&gt;Probe Dock RT Arquillian Extension&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#java-category-by-package&quot;&gt;Java Probes Category by Package&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bugfixes&quot;&gt;Bugfixes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;report-permalinks&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;report-permalinks&quot;&gt;Report Permalinks&lt;/h2&gt;

&lt;p&gt;It is now possible to build web links to an automated test run report from a continuous integration environment, such as a nightly build.
You can send these links in an e-mail to your developers so that they may more easily access the latest reports.&lt;/p&gt;

&lt;p&gt;For now, this feature is limited.
There are two requirements to be able to build a link to a report:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;You must get the ID of your organization, which you can find in the edition dialog (you must be an organization administrator).&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/media/probedock-v0_1_11/report-permalinks-org-id.jpg&quot; alt=&quot;Getting your organization ID&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You must submit your test result payloads with an explicit report identifier (UID).&lt;/p&gt;

    &lt;p&gt;You might already be doing this if you are grouping multiple test runs (e.g. from different test frameworks) under one report.
Basically, you must generate a unique identifier for the report before you run the tests, and set the &lt;code&gt;$PROBEDOCK_TEST_REPORT_UID&lt;/code&gt; environment variable
&lt;a href=&quot;https://github.com/probedock/probedock-probes#test-run-report-configuration&quot;&gt;as described here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once you have this information, you can build a link that will redirect you to the report once it has been generated.
The format of the link is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://demo.probedock.io/go/report?organizationId=ORG_ID&amp;amp;uid=UID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, one of the latest Probe Dock reports testing this feature has the UID &lt;code&gt;20151218131619-04ed9e53-4eab-421a-9895-0691cb049ba8&lt;/code&gt;, and the Probe Dock organization ID is &lt;code&gt;6bhni&lt;/code&gt;. The full permalink to that report is therefore:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://demo.probedock.io/go/report?organizationId=6bhni&amp;amp;uid=20151218131619-04ed9e53-4eab-421a-9895-0691cb049ba8&quot;&gt;https://demo.probedock.io/go/report?organizationId=6bhni&amp;amp;uid=20151218131619-04ed9e53-4eab-421a-9895-0691cb049ba8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that it might take a minute for the link to work after you have submitted the test result payloads,
as Probe Dock might still be building the full report.
But once it’s ready, it will serve as a permalink, still working even if you rename your organization.&lt;/p&gt;

&lt;p&gt;In the future, we will update the Probe Dock API, UI and probes so that they will provide the permalinks, saving you the trouble of building them.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;junit-assumptions-and-ignored-tests&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;junit-assumptions-support--ignored-tests&quot;&gt;JUnit Assumptions Support &amp;amp; Ignored Tests&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/probedock/probedock-junit&quot;&gt;JUnit Probe&lt;/a&gt; has been updated to support &lt;a href=&quot;https://github.com/junit-team/junit/wiki/Assumptions-with-assume&quot;&gt;assumption failures&lt;/a&gt; and ignored tests.
Both will be sent to Probe Dock but flagged as inactive (shown in yellow).&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/probedock/probedock-rt-junit&quot;&gt;JUnit RT Probe&lt;/a&gt; has also been updated to support these features.
These two events prevent the results from being sent to Probe Dock RT.
But they can be sent when filtering is enabled depending on the testing framework used (e.g. Arquillian).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;rt-arquillian&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;probe-dock-rt-arquillian-extension&quot;&gt;Probe Dock RT Arquillian Extension&lt;/h2&gt;

&lt;p&gt;A &lt;a href=&quot;https://github.com/probedock/probedock-rt-junit&quot;&gt;Probe Dock RT Arquillian Extension&lt;/a&gt; is available to enable test execution filtering.
You do not have to use it for your Arquillian tests but it is required if you want to filter test execution.&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://github.com/probedock/probedock-demo-arquillian&quot;&gt;Arquillian integration demo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;java-category-by-package&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;java-probes-category-by-package&quot;&gt;Java Probes Category by Package&lt;/h2&gt;

&lt;p&gt;Java-based probes have a new feature that allows you to change the Probe Dock category of your tests depending on which package they are in.
This is done through configuration in your &lt;code&gt;probedock.yml&lt;/code&gt; file.
For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java:
  categoriesByPackage:
    io.probedock.integration.*: Integration
    io.probedock.api**: API
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The pattern matching is based on minimatch.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;bugfixes&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bugfixes&quot;&gt;Bugfixes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Fixed the shell scripts generated by the Getting Started page to be compatible with Bash.&lt;/li&gt;
  &lt;li&gt;Fixed a bug where sending a &lt;code&gt;null&lt;/code&gt; category would cause the test payload processing to fail.&lt;/li&gt;
  &lt;li&gt;Fixed a bug where that allowed anonymous users to see private organizations in the list of organizations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;merry-christmas--happy-new-year&quot;&gt;Merry Christmas &amp;amp; Happy New Year!&lt;/h2&gt;
</description>
        <pubDate>Fri, 18 Dec 2015 16:00:00 +0100</pubDate>
        <link>http://probedock.io/blog/releases/2015/12/18/probedock-v0_1_11/</link>
        <guid isPermaLink="true">http://probedock.io/blog/releases/2015/12/18/probedock-v0_1_11/</guid>
        
        
        <category>releases</category>
        
      </item>
    
      <item>
        <title>Integrating Probe Dock and Probe Dock RT into a Java EE project</title>
        <description>&lt;p&gt;When you build a Java EE project, you can use a variety of automated tests. Unit tests, integration tests, tests for your REST API. It is also quite common to have functional tests for the web user interface.&lt;/p&gt;

&lt;p&gt;We have prepared a full stack Java EE demo project where you can discover step by step how to integrate multiple test technologies with Probe Dock.&lt;/p&gt;

&lt;h3 id=&quot;system-under-test&quot;&gt;System under test&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;system&lt;/strong&gt; under test is an object-oriented calculator with four operations: &lt;code&gt;div&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt; and &lt;code&gt;add&lt;/code&gt;. Its REST API offers a simple language to express more complex operation (e.g. &lt;code&gt;(2 + (3 - 1))&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://github.com/probedock/probedock-demo-jee-stack/tree/master/probedock-demo-jee-stack-notest&quot;&gt;demo project (without tests)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here’s a sample payload for the REST API.
It represents the calculation &lt;code&gt;2 + (10 - ((15 / 3) * 4))&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;add&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;quot;left&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;quot;rightOperation&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;sub&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;quot;left&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;quot;rightOperation&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;mul&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;quot;leftOperation&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;div&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;quot;left&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;quot;right&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;quot;right&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Submitting this payload with a POST request on &lt;code&gt;http://localhost:8080/jee-stack/api/calculator&lt;/code&gt; will yield the following result:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;quot;result&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;-8&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;unit-tests&quot;&gt;Unit tests&lt;/h3&gt;

&lt;p&gt;In the &lt;a href=&quot;https://github.com/probedock/probedock-demo-jee-stack/tree/master/probedock-demo-jee-stack-unit&quot;&gt;probedock-demo-jee-stack-unit&lt;/a&gt; project, we added JUnit tests as the first test technology in our stack. We have already integrated Probe Dock and Probe Dock RT into this project. Go through the README to see exactly what we did.&lt;/p&gt;

&lt;p&gt;This is a short summary of modifications compared to the initial project without tests:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Added Probe Dock and test dependencies&lt;/li&gt;
  &lt;li&gt;Configured Maven Surefire plugin for Probe Dock&lt;/li&gt;
  &lt;li&gt;Wrote a few unit tests with JUnit&lt;/li&gt;
  &lt;li&gt;Added the &lt;code&gt;probedock.yml&lt;/code&gt; project configuration file&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;integration-tests&quot;&gt;Integration tests&lt;/h3&gt;

&lt;p&gt;In the &lt;a href=&quot;https://github.com/probedock/probedock-demo-jee-stack/tree/master/probedock-demo-jee-stack-integration&quot;&gt;probedock-demo-jee-stack-integration&lt;/a&gt; project, we introduced integration tests with &lt;a href=&quot;http://arquillian.org&quot;&gt;Arquillian&lt;/a&gt; in addition to unit tests.&lt;/p&gt;

&lt;p&gt;Arquillian is a Java EE test framework which runs the tests in a lightweight application container where all Java EE components are managed as you are used to.&lt;/p&gt;

&lt;p&gt;This is a summary of modifications compared to the initial project, including the previous setup of the unit tests:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Added Probe Dock and test (JUnit, Arquillian, …) dependencies&lt;/li&gt;
  &lt;li&gt;Added the dependency management configuration for Arquillian for the version management&lt;/li&gt;
  &lt;li&gt;Configured Maven Surefire plugin for Probe Dock&lt;/li&gt;
  &lt;li&gt;Added several configuration files required by Arquillian&lt;/li&gt;
  &lt;li&gt;We have written a few integration tests with JUnit and added the required annotations and methods for Arquillian. Each test class will have the responsibility to create its own archive to be run by Arquillian.&lt;/li&gt;
  &lt;li&gt;Added the &lt;code&gt;probedock.yml&lt;/code&gt; project configuration file&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;api-tests&quot;&gt;API tests&lt;/h3&gt;

&lt;p&gt;In the &lt;a href=&quot;https://github.com/probedock/probedock-demo-jee-stack/tree/master/probedock-demo-jee-stack-api&quot;&gt;probedock-demo-jee-stack-api&lt;/a&gt; project, we introduced API tests. We used &lt;a href=&quot;https://github.com/probedock/java-api-test&quot;&gt;java-api-test&lt;/a&gt; to write our API tests.&lt;/p&gt;

&lt;p&gt;The test framework offers an abstraction to make JSON requests on a REST API. It also provides utility methods to make assertions on JSON responses.&lt;/p&gt;

&lt;p&gt;We have also used another library to manage data through JPA: &lt;a href=&quot;https://github.com/probedock/junitee-data-utils&quot;&gt;junitee-data-utils&lt;/a&gt;. This library allows us to populate data through code in the test setup phase.&lt;/p&gt;

&lt;p&gt;With Arquillian, there are two modes to run the tests: the server mode and the client mode. We use the server mode for integration tests, and the client mode for API tests. In the client mode, the tests are run &lt;em&gt;outside&lt;/em&gt; of the application context. Therefore, we cannot inject services in tests directly as they are not managed.&lt;/p&gt;

&lt;p&gt;This is a summary of modifications compared to the initial project, including the previous setups of the unit and integration tests:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Added Probe Dock and test (JUnit, java-api-test, …) dependencies&lt;/li&gt;
  &lt;li&gt;Added the dependency management configuration for Arquillian for the version management&lt;/li&gt;
  &lt;li&gt;Configured Maven Surefire plugin for Probe Dock&lt;/li&gt;
  &lt;li&gt;Added several configuration files required by Arquillian&lt;/li&gt;
  &lt;li&gt;We have written a few integration tests with JUnit and added the required annotations and methods for Arquillian. Each test class will have the responsibility to create its own archive to be run by Arquillian. In client mode, we also benefit from some injections like the URL where the application is running by Arquillian.&lt;/li&gt;
  &lt;li&gt;Added the &lt;code&gt;probedock.yml&lt;/code&gt; project configuration file&lt;/li&gt;
  &lt;li&gt;Added the configuration and code to integrate junitee-data-utils&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;rule-them-all&quot;&gt;Rule them all&lt;/h3&gt;

&lt;p&gt;Finally, we bring all these pieces together. The project &lt;a href=&quot;https://github.com/probedock/probedock-demo-jee-stack/tree/master/probedock-demo-jee-stack-all&quot;&gt;probedock-demo-jee-stack-all&lt;/a&gt; contains the three kinds of tests and a way to run each of them separately or all together.&lt;/p&gt;

&lt;p&gt;We performed the following steps to have all tests running correctly:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We have combined all the tests into the same project:
    &lt;ul&gt;
      &lt;li&gt;We have the standard packages for unit tests.&lt;/li&gt;
      &lt;li&gt;We have the &lt;code&gt;integration&lt;/code&gt; package for the integration tests.&lt;/li&gt;
      &lt;li&gt;We have the &lt;code&gt;api&lt;/code&gt; package for the API tests.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;We combined the different configuration files.&lt;/li&gt;
  &lt;li&gt;We have added the probedock project configuration file with the correct categories by package to make sure integration and API tests are assigned the correct category in Probe Dock.&lt;/li&gt;
  &lt;li&gt;We have tuned the Maven Surefire plugin configuration to run only the unit tests by default when running the command &lt;code&gt;mvn clean install&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;We added three maven profiles with custom Maven Surefire configurations:
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;integration&lt;/code&gt; to run only integration tests with &lt;code&gt;mvn clean install -Pintegration&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;api&lt;/code&gt; to run only API tests tests with &lt;code&gt;mvn clean install -Papi&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;all&lt;/code&gt; to run all tests at once with &lt;code&gt;mvn clean install -Pall&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Hopefully, this article and the companion &lt;a href=&quot;https://github.com/probedock/probedock-demo-jee-stack&quot;&gt;GitHub repo&lt;/a&gt; have given you a good understanding of what needs to be done in order to integrate different test technologies in a Java EE application. Use our demo project to learn how to integrate Probe Dock into your Java EE projects or any Java application with JUnit-based testing.&lt;/p&gt;

&lt;p&gt;If you use other test frameworks based on JUnit, they will be supported out of the box by the &lt;a href=&quot;https://github.com/probedock/probedock-junit&quot;&gt;Probe Dock JUnit probe&lt;/a&gt;. You simply need to add the listener in the Maven Surefire plugin configuration.&lt;/p&gt;
</description>
        <pubDate>Thu, 17 Dec 2015 15:00:00 +0100</pubDate>
        <link>http://probedock.io/blog/howto/2015/12/17/probedock-javaee-demo/</link>
        <guid isPermaLink="true">http://probedock.io/blog/howto/2015/12/17/probedock-javaee-demo/</guid>
        
        
        <category>howto</category>
        
      </item>
    
      <item>
        <title>Probe Dock is awarded a FIT grant to accelerate innovation in agile testing</title>
        <description>&lt;p&gt;It is a pleasure and a pride for us to announce that Probe Dock has just been awarded a prestigious innovation grant by the Foundation for Technological Innovation (&lt;a href=&quot;http://www.fondation-fit.ch/&quot;&gt;FIT&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This financial support will allow us to accelerate the development of what we believe is a truly original agile testing platform. In collaboration with the University of Applied Sciences of Western Switzerland, we will be working hard to make progress towards our long-term vision.&lt;/p&gt;

&lt;h3 id=&quot;a-software-quality-platform-crafted-for-agile-teams&quot;&gt;A software quality platform crafted for agile teams&lt;/h3&gt;

&lt;p&gt;For a very long time, software development and quality assurance have been managed as separate and sequential activities, carried out by different teams. It is now generally accepted that quality should not be assessed after the fact. In the contrary, it should be baked into the product and under the responsibility of a single team working closely together.&lt;/p&gt;

&lt;p&gt;Despite this awareness, the reality is that most testing tools and QA platforms have not addressed this paradigm shift. They have been designed under the premise that quality should be &lt;em&gt;controlled externally&lt;/em&gt;. They have not been designed to provide information and feedback directly to the team, on a continuous basis. To make a blunt statement, they support the vision that software development should be managed with a &lt;em&gt;Command and Control (and Punish)&lt;/em&gt; philosophy.&lt;/p&gt;

&lt;p&gt;Probe Dock takes a new stand on software quality. From its inception, the platform has been created to address the needs of agile teams. More fundamentally, it has been designed to foster agile values:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;feedback and transparency&lt;/li&gt;
  &lt;li&gt;introspection and continuous improvement&lt;/li&gt;
  &lt;li&gt;courage and pride&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our goal is to offer a platform to agile, multi-disciplinary teams that support their continuous improvement process, by providing them with detailed feedback in original ways. Our goal is to provide them with insights not only about the quality of their product, but also about the efficiency of their engineering and collaborative practices.&lt;/p&gt;

&lt;h3 id=&quot;the-road-ahead&quot;&gt;The road ahead&lt;/h3&gt;

&lt;p&gt;The first step towards our vision for Probe Dock has been to offer an efficient and uniform way to collect test reports, across system components, technologies and build environments.&lt;/p&gt;

&lt;p&gt;Today, we have a solution that makes it easier to deal with the data generated by heterogenous testing frameworks. No need to chase test reports in different places. No more struggle to extract information from many diverse report formats. Probe Dock makes it a breeze to place probes in your existing projects, to collect normalized reports in a central location and to get insights from quality dashboards.&lt;/p&gt;

&lt;p&gt;The vast amount of information continuously collected by the probes truly is a gold mine. Our vision for Probe Dock is to exploit it, with the objective to provide new insights to development teams. With this in mind, we are actively developing Probe Dock on a number of different fronts. While this is not the place to describe our roadmap in details, we can mention three areas where you can expect to hear more from us in the coming months : &lt;em&gt;deep analytics&lt;/em&gt;, &lt;em&gt;gamification&lt;/em&gt; and &lt;em&gt;ambient interfaces&lt;/em&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 19 Nov 2015 14:00:00 +0100</pubDate>
        <link>http://probedock.io/blog/roadmap/2015/11/19/fitGrant/</link>
        <guid isPermaLink="true">http://probedock.io/blog/roadmap/2015/11/19/fitGrant/</guid>
        
        
        <category>roadmap</category>
        
      </item>
    
      <item>
        <title>Using Probe Dock with Jasmine 2</title>
        <description>&lt;p&gt;Are you using Jasmine 2 and do you want to send your test results to Probe Dock?&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&quot;/blog/howto/2015/10/11/probedock-integration-with-mocha/&quot;&gt;previous post&lt;/a&gt;, we have shown how the &lt;a href=&quot;https://www.npmjs.com/package/probedock-node&quot;&gt;probedock-node&lt;/a&gt; npm module can be used to send test results to the Probe Dock server in a JavaScript environment. To illustrate the process, we have shown what you need to do if you are using the Mocha framework. You can grab the code in &lt;a href=&quot;https://github.com/probedock/probedock-demo-mocha&quot;&gt;this GitHub repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We have now done the same thing for Jasmine2. If you are using this framework to write your tests, you will see that you can very easily collect the results (using a custom reporter), prepare a test run payload and send it to the Probe Dock server. For the details, have a look in &lt;a href=&quot;https://github.com/probedock/probedock-demo-jasmine2&quot;&gt;this repo&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/2015-10-24-probedock-with-jasmine2/server-01.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Oct 2015 15:00:00 +0200</pubDate>
        <link>http://probedock.io/blog/roadmap/2015/10/24/probedock-with-jasmine2/</link>
        <guid isPermaLink="true">http://probedock.io/blog/roadmap/2015/10/24/probedock-with-jasmine2/</guid>
        
        
        <category>roadmap</category>
        
      </item>
    
      <item>
        <title>Probe Dock Soft-Shake Presentation</title>
        <description>&lt;p&gt;We presented Probe Dock at &lt;a href=&quot;http://soft-shake.ch/2015/en/&quot;&gt;the Soft-Shake 2015&lt;/a&gt; this morning.
It went very well and we got interesting questions and feedback.
Thanks to all attendees and of course to the conference sponsors and organizers!&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://www.dropbox.com/s/krwp0gowh57t7xa/Soft-Shake%202015%20-%20Test%20analytics%20presentation.pdf?dl=0&quot;&gt;slides we used for the presentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As always, do not hesitate to contact us if you have additional questions or feedback.&lt;/p&gt;

&lt;h2 id=&quot;q--a-after-the-presentation&quot;&gt;Q &amp;amp; A after the presentation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Does Probe Dock include code coverage information?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Not at this time but it’s clearly something that we plan to add.
We can obtain code coverage metrics through integrations with other tools (e.g. Sonar, Coveralls), that we will analyze and correlate with other test data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Does Probe Dock display information about the machine/environment on which the tests are executed?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We already collect this information but don’t have it in the UI yet.
It’s definitely information that will be useful to help solve &lt;em&gt;“Works on my machine”&lt;/em&gt; kinds of problems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Can Probe Dock show a build matrix (i.e. when tests are executed multiple times on different combinations of OS, browser, dependency versions, etc)?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Probe Dock can collect the results of a build matrix in a report, but it currently won’t collect information about the matrix itself.
A limited workaround at this time is to use tags to differentiate between the test results of each execution.
Including more advanced build matrix features is on our roadmap but not necessarily a priority right now&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How can Probe Dock differentiate between the different components of my big project?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The current Probe Dock model allows you to manage &lt;em&gt;projects&lt;/em&gt;.
Each Probe Dock project might correspond to an entire project or yours, or to only one component in your project.
You can organize them as you see fit and decide what goes into each report.&lt;/p&gt;

&lt;p&gt;We have been discussing more advanced project/component models that will provide more flexibility, but nothing definite has been decided yet.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Jenkins can also display automated test reports. What does Probe Dock offer that Jenkins doesn’t already have?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Jenkins does have the ability to display test results in a report, but little more,
and it is very much Java/Maven-oriented whereas Probe Dock aims to support any testing framework that has listeners (most of them do).&lt;/p&gt;

&lt;p&gt;Probe Dock also tracks individual tests over time,
allows developers to enrich tests with custom data freely defined by you (tags &amp;amp; tickets),
and tracks day-to-day developer activity in addition to continuous integration pipelines.&lt;/p&gt;

&lt;p&gt;Its goal is to offer many more insights into testing trends and team behavior,
and to correlate the data of testing activities with other data such as test coverage and bug reports.&lt;/p&gt;
</description>
        <pubDate>Thu, 22 Oct 2015 22:00:00 +0200</pubDate>
        <link>http://probedock.io/blog/news/2015/10/22/probedock-soft-shake-presentation/</link>
        <guid isPermaLink="true">http://probedock.io/blog/news/2015/10/22/probedock-soft-shake-presentation/</guid>
        
        
        <category>news</category>
        
      </item>
    
      <item>
        <title>Integrating Probe Dock with Mocha</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://mochajs.org/&quot;&gt;Mocha&lt;/a&gt; is one of the most popular JavaScript testing frameworks. We have integrated it with Probe Dock, so that test results can be collected and analyzed via our web interface.&lt;/p&gt;

&lt;p&gt;Mocha provides a lot of very nice features, which make testing efficient and enjoyable. The framework allows you to test both server-side and client-side components. It supports asynchronous tests, custom reporters and much more.&lt;/p&gt;

&lt;p&gt;To illustrate how Probe Dock can be integrated with existing testing frameworks, we have implemented a reference project where automated tests are written and executed with Mocha and where test results are sent to Probe Dock.&lt;/p&gt;

&lt;p&gt;Have a look at &lt;a href=&quot;https://github.com/probedock/probedock-demo-mocha&quot;&gt;this GitHub repo&lt;/a&gt; to see the result and get access to our Mocha runner. If you are using this framework, you will be able to collect your test results and feed them in your Probe Dock server.&lt;/p&gt;

&lt;h3 id=&quot;what-is-our-system-under-test&quot;&gt;What is our System-Under-Test?&lt;/h3&gt;

&lt;p&gt;In this demo, we consider &lt;strong&gt;two different systems&lt;/strong&gt; that we want to validate with our Mocha tests:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;the first one is &lt;strong&gt;a very simple Node.js module&lt;/strong&gt;, which exposes two functions. We are going to check that the functions return values of the expected type.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the second one is &lt;strong&gt;a fictive distributed application&lt;/strong&gt; (with a front-end, a back-end, etc.). There is actually no code at all: we only want to show that with Mocha, it is possible to recursively define test suites within test suites.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the code of the node module that we will test:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * This is the first System-Under-Test for our demo. It is a &lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * very basic node module, which exposes two functions. They&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * both return a string value. The first function is synchronous,&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * the second one is asynchronous.&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; */&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;exports&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;doSomething&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;sweet&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;exports&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;doSomethingAsynchronously&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;nx&quot;&gt;setTimeout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;nx&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cool&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;write-tests-with-mocha&quot;&gt;Write tests with Mocha&lt;/h3&gt;

&lt;p&gt;Writing tests with Mocha is easy. You declare &lt;strong&gt;test suites&lt;/strong&gt; by calling the &lt;code&gt;describe&lt;/code&gt; function. You then declare &lt;strong&gt;individual tests&lt;/strong&gt; by calling the &lt;code&gt;it&lt;/code&gt; function. In the callback function that you pass to &lt;code&gt;it&lt;/code&gt;, you invoke the System-Under-Test (e.g. you make calls to your Node.js module) and then make assertions on the provided results.&lt;/p&gt;

&lt;p&gt;Here is the test suite for validating the behavior of our simple Node.js module:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * Let&amp;#39;s use the standard node.js module &amp;quot;assert&amp;quot; for making assertions. &lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * We could use a richer framework, such as should.js or chai.&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; */&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;assert&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;


&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * Load the code that we want to test. It is a simple node module, &lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * which exposes two functions.&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; */&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;../app/index.js&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;


&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * Create a test suite for our &amp;quot;app&amp;quot; node module.&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; */&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;app&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

	&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;	 * Within the module test suite, create a test suite for &lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;	 * the &amp;quot;doSomething()&amp;quot; function&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;	 */&lt;/span&gt;
	&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;doSomething&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

		&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;		 * Define a first test for the function&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;		 */&lt;/span&gt;
		&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;shouldReturnAString&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
			&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;doSomething&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
			&lt;span class=&quot;nx&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;string&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typeof&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

		&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;		 * Define a second test for the function&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;		 */&lt;/span&gt;
		&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;shouldReturnSweet&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
			&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;doSomething&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
			&lt;span class=&quot;nx&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;sweet&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

	&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;	 * Within the module test suite, create a test suite for the &lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;	 * &amp;quot;doSomethingAsynchronously()&amp;quot; function.&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;	 */&lt;/span&gt;
	&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;doSomethingAsynchronously&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;shouldReturnCool&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;nx&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;doSomethingAsynchronously&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
				&lt;span class=&quot;nx&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cool&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
				&lt;span class=&quot;nx&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And here is the test suite for validating the behavior of our fictive distributed application:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * Let&amp;#39;s use the standard node.js module &amp;quot;assert&amp;quot; for making assertions. We&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * could use a richer framework, such as should.js or chai.&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; */&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;assert&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;


&lt;span class=&quot;cm&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * Create a test suite for a distributed application, composed of several&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; * sub-systems. This shows how test suites can be nested in multiple levels.&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt; */&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;A distributed application&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

	&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The backend sub-system&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

		&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The REST API&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;	
			&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;should work&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{});&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

		&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The business services&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;	
			&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;should work&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{});&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

		&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The data access layer&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;	
			&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;should work&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Technical error while talking to the database.&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

	&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The front-end sub-system&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The admin interface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;	
			&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The monitoring pages&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;	
				&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;should work&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{});&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
			&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The configuration pages&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;	
				&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;should work&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{});&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
		&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The end-user interface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;	
			&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The account pages&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;	
				&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;should work&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
					&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;hasWorked&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
					&lt;span class=&quot;nx&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hasWorked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
				&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
			&lt;span class=&quot;nx&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The fun pages&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;	
				&lt;span class=&quot;nx&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;should work&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{});&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;configure-your-probe-dock-environment&quot;&gt;Configure your Probe Dock environment&lt;/h3&gt;

&lt;p&gt;To use Probe Dock, you can install your own server. You can also get a free trial account on our SaaS platform. You will find all details on &lt;a href=&quot;http://probedock.io/getting-started/&quot;&gt;this page&lt;/a&gt;. After registration, you will get &lt;strong&gt;credentials&lt;/strong&gt; that you will need to store in a local configuration file (see below).&lt;/p&gt;

&lt;p&gt;Once you have access to a server, you will also need to &lt;strong&gt;create a new Probe Dock project&lt;/strong&gt; and get its &lt;strong&gt;API key&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Finally, you will need to make sure that &lt;strong&gt;two configuration files&lt;/strong&gt; are properly setup on your machine. Firstly, in &lt;code&gt;~/.probedock/probedock.yml&lt;/code&gt;, you will need to enter the URL of your server and your user credentials. Secondly, in your project folder, you will need a &lt;code&gt;probedock.yml&lt;/code&gt; file with the API key of your project.&lt;/p&gt;

&lt;h3 id=&quot;run-the-tests&quot;&gt;Run the tests&lt;/h3&gt;

&lt;p&gt;To run the tests, use this command: &lt;code&gt;node probedock-mocha-runner.js&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In our implementation, we have used the &lt;strong&gt;Mocha programmatic API&lt;/strong&gt; to control the Mocha runner and be notified of test successes and failures. Once all tests have been executed, we send the results to the Probe Dock server with our &lt;a href=&quot;https://www.npmjs.com/package/probedock-node&quot;&gt;probedock-node&lt;/a&gt; library. This is the npm module that you should use if you want to integrate Probe Dock in your development workflow and want to have a lot of control and flexibility.&lt;/p&gt;

&lt;p&gt;When you run the command, you should see the following output on your console. Note that two tests are expected to fail (we generate exceptions on purpose).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/2015-10-17-probedock-integration-with-mocha/console-01.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;see-the-test-results-in-probe-dock&quot;&gt;See the test results in Probe Dock&lt;/h3&gt;

&lt;p&gt;If your setup is correct, you should see the test results in the Probe Dock web interface.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/2015-10-17-probedock-integration-with-mocha/server-01.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/2015-10-17-probedock-integration-with-mocha/server-02.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-do-all-pieces-fit-together&quot;&gt;How do all pieces fit together?&lt;/h3&gt;

&lt;p&gt;You will find more details about the implementation on our &lt;a href=&quot;https://github.com/probedock/probedock-demo-mocha&quot;&gt;GitHub repo&lt;/a&gt;, but the following diagram should already give you a pretty good idea about how Probe Dock, Mocha and your code base fit together.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/2015-10-17-probedock-integration-with-mocha/architecture.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Hopefully, this article and the companion &lt;a href=&quot;https://github.com/probedock/probedock-demo-mocha&quot;&gt;GitHub repo&lt;/a&gt; have given you a good understanding about what needs to be done in order to post test results from a JavaScript environment. If you are using Mocha, then you can use our code either by invoking our runner script from the command line, or by integrating in your favorite build tool (e.g. Grunt, Gulp).&lt;/p&gt;

&lt;p&gt;If you are using another test framework, it should not be too difficult to adapt our code to your needs. It is almost always about using the test framework to launch a runner, listen to test result events and ask the Probe Dock client to send the results to the server.&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Oct 2015 15:00:00 +0200</pubDate>
        <link>http://probedock.io/blog/howto/2015/10/11/probedock-integration-with-mocha/</link>
        <guid isPermaLink="true">http://probedock.io/blog/howto/2015/10/11/probedock-integration-with-mocha/</guid>
        
        
        <category>howto</category>
        
      </item>
    
  </channel>
</rss>
